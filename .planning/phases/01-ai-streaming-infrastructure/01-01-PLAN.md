---
phase: 01-ai-streaming-infrastructure
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - sidekiq-webapp/package.json
  - sidekiq-webapp/src/env.js
  - sidekiq-webapp/src/lib/ai/models.ts
  - sidekiq-webapp/src/lib/ai/gateway.ts
  - sidekiq-webapp/src/lib/validations/chat.ts
  - sidekiq-webapp/src/app/api/chat/route.ts
autonomous: true
user_setup:
  - service: vercel-ai-gateway
    why: "AI Gateway requires API key for LLM access"
    env_vars:
      - name: AI_GATEWAY_API_KEY
        source: "Vercel Dashboard -> AI -> Gateway -> API Keys -> Create"
    dashboard_config: []

must_haves:
  truths:
    - "POST /api/chat returns streaming SSE response"
    - "User message is saved to database immediately on request"
    - "AI message is saved to database after stream completes"
    - "Token usage is tracked from provider response"
    - "Stream continues even if client disconnects (consumeStream)"
  artifacts:
    - path: "sidekiq-webapp/src/lib/ai/models.ts"
      provides: "Model registry with all supported providers"
      exports: ["AVAILABLE_MODELS", "DEFAULT_MODEL", "ModelConfig", "getModel"]
    - path: "sidekiq-webapp/src/lib/ai/gateway.ts"
      provides: "AI Gateway instance"
      exports: ["gateway"]
    - path: "sidekiq-webapp/src/lib/validations/chat.ts"
      provides: "Zod schemas for chat requests"
      exports: ["chatRequestSchema"]
    - path: "sidekiq-webapp/src/app/api/chat/route.ts"
      provides: "Streaming chat endpoint"
      exports: ["POST"]
  key_links:
    - from: "sidekiq-webapp/src/app/api/chat/route.ts"
      to: "sidekiq-webapp/src/lib/ai/gateway.ts"
      via: "import gateway, call streamText"
      pattern: "gateway\\("
    - from: "sidekiq-webapp/src/app/api/chat/route.ts"
      to: "sidekiq-webapp/src/server/db/schema.ts"
      via: "import messages, insert in onFinish"
      pattern: "db\\.insert\\(messages\\)"
---

<objective>
Create the AI streaming backend infrastructure using Vercel AI SDK with AI Gateway for multi-provider LLM access.

Purpose: Establish the server-side foundation for streaming AI responses with reliable message persistence. This is the core pipeline that all chat features depend on.

Output:
- AI Gateway integration with model registry
- Streaming Route Handler at `/api/chat`
- Message persistence with token tracking
- Zod validation for chat requests
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-ai-streaming-infrastructure/01-RESEARCH.md
@.planning/phases/01-ai-streaming-infrastructure/01-CONTEXT.md
@sidekiq-webapp/src/server/db/schema.ts
@sidekiq-webapp/src/server/db/index.ts
@sidekiq-webapp/src/env.js
@sidekiq-webapp/package.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install AI SDK packages and configure environment</name>
  <files>
    sidekiq-webapp/package.json
    sidekiq-webapp/src/env.js
  </files>
  <action>
Install the Vercel AI SDK packages:
```bash
cd sidekiq-webapp && pnpm add ai @ai-sdk/react @ai-sdk/gateway nanoid
```

Update `src/env.js` to add the AI Gateway API key:
- Add `AI_GATEWAY_API_KEY: z.string()` to server schema
- Add to runtimeEnv mapping

Note: nanoid is for generating server-side message IDs.
  </action>
  <verify>
Run `pnpm install` completes without errors.
Run `pnpm check` passes type checking.
Check package.json includes `ai`, `@ai-sdk/react`, `@ai-sdk/gateway`, `nanoid`.
  </verify>
  <done>AI SDK packages installed, env schema updated with AI_GATEWAY_API_KEY</done>
</task>

<task type="auto">
  <name>Task 2: Create model registry and AI Gateway instance</name>
  <files>
    sidekiq-webapp/src/lib/ai/models.ts
    sidekiq-webapp/src/lib/ai/gateway.ts
  </files>
  <action>
Create `src/lib/ai/gateway.ts`:
- Import `createGateway` from `@ai-sdk/gateway`
- Create and export `gateway` instance using `AI_GATEWAY_API_KEY` from env
- This is a server-only file (add 'use server' or import from server-only)

Create `src/lib/ai/models.ts`:
- Define `ModelConfig` type with: id (string), name (display name), provider ('openai' | 'anthropic' | 'google'), pricingTier ('$' | '$$' | '$$$'), speedTier ('fast' | 'balanced' | 'quality')
- Export `AVAILABLE_MODELS` array with these models:
  - OpenAI: gpt-4o-mini ($, fast), gpt-4o ($$, balanced), o1 ($$$, quality)
  - Anthropic: claude-3-5-haiku-latest ($, fast), claude-sonnet-4-20250514 ($$, balanced), claude-opus-4-20250514 ($$$, quality)
  - Google: gemini-2.0-flash ($, fast), gemini-2.5-pro-preview-05-06 ($$$, quality)
- Export `DEFAULT_MODEL = 'anthropic/claude-sonnet-4-20250514'`
- Export `getModel(modelId: string)` function that returns `gateway(modelId)`
- Export `getModelConfig(modelId: string)` to lookup model metadata

Use model IDs in format: `provider/model-name` (e.g., `openai/gpt-4o`).
  </action>
  <verify>
Files exist at specified paths.
TypeScript compiles without errors: `pnpm typecheck`
Imports resolve correctly.
  </verify>
  <done>Model registry exports AVAILABLE_MODELS, DEFAULT_MODEL, getModel. Gateway instance configured.</done>
</task>

<task type="auto">
  <name>Task 3: Create chat Route Handler with streaming and persistence</name>
  <files>
    sidekiq-webapp/src/lib/validations/chat.ts
    sidekiq-webapp/src/app/api/chat/route.ts
  </files>
  <action>
Create `src/lib/validations/chat.ts`:
- Define `chatRequestSchema` with Zod:
  - `messages`: array of UIMessage format (from AI SDK)
  - `threadId`: string (required)
  - `model`: string (optional, defaults to DEFAULT_MODEL)
- Export the schema and inferred type

Create `src/app/api/chat/route.ts`:
- Import: `streamText`, `convertToModelMessages`, `UIMessage` from 'ai'
- Import: `gateway` from lib/ai/gateway, `getModel` from lib/ai/models
- Import: `db` from server/db, `messages`, `threads` from server/db/schema
- Import: `nanoid` for server-generated IDs
- Import: `getSession` from server/better-auth/server for auth
- Import: `eq` from drizzle-orm for query building

POST handler logic:
1. Validate session (return 401 if not authenticated)
2. Parse and validate request body with chatRequestSchema
3. Extract last user message and save immediately to database:
   ```typescript
   const userMessage = uiMessages[uiMessages.length - 1];
   const userMessageId = nanoid();
   await db.insert(messages).values({
     id: userMessageId,
     threadId,
     role: 'user',
     content: extractTextContent(userMessage),
     createdAt: new Date(),
   });
   ```
4. Track start time for latency calculation
5. Call `streamText` with:
   - `model`: gateway(modelId)
   - `messages`: await convertToModelMessages(uiMessages)
   - `abortSignal`: req.signal
6. Call `result.consumeStream()` immediately (CRITICAL for persistence)
7. Return `result.toUIMessageStreamResponse()` with:
   - `originalMessages`: uiMessages
   - `generateMessageId`: () => nanoid()
   - `onFinish`: async callback that:
     - Calculates latency
     - Gets assistant message from finalMessages
     - Extracts text content from message parts
     - Inserts assistant message to database with model, inputTokens, outputTokens, metadata (finishReason, latencyMs, aborted flag)
     - Updates thread.lastActivityAt

Helper function `extractTextContent(message: UIMessage)`:
- Filter message.parts for type === 'text'
- Map to text property and join with empty string

Do NOT use Edge runtime (need database access). Default Node.js runtime is correct.
  </action>
  <verify>
TypeScript compiles: `pnpm typecheck`
Route file exists at `src/app/api/chat/route.ts`
Manual test (after env setup):
```bash
curl -X POST http://localhost:3000/api/chat \
  -H "Content-Type: application/json" \
  -H "Cookie: <session-cookie>" \
  -d '{"threadId":"test","messages":[{"id":"1","role":"user","parts":[{"type":"text","text":"Hello"}]}]}'
```
Should return streaming response (SSE format).
  </verify>
  <done>
Route Handler streams AI responses, saves user message immediately, saves AI message with tokens in onFinish, updates thread activity.
  </done>
</task>

</tasks>

<verification>
1. All files created at specified paths
2. `pnpm typecheck` passes
3. `pnpm check` (lint + typecheck) passes
4. Package.json includes ai, @ai-sdk/react, @ai-sdk/gateway, nanoid
5. env.js includes AI_GATEWAY_API_KEY schema
6. Route handler properly authenticated (returns 401 without session)
</verification>

<success_criteria>
- AI SDK packages installed and importable
- Model registry provides type-safe model configuration
- `/api/chat` Route Handler exists and exports POST
- Request validation uses Zod schemas
- User messages saved immediately on request
- AI messages saved in onFinish with token tracking
- `consumeStream()` called before returning response
- Stream abort handled via req.signal
</success_criteria>

<output>
After completion, create `.planning/phases/01-ai-streaming-infrastructure/01-01-SUMMARY.md`
</output>
